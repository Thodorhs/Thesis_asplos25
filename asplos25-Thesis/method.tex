\section{Experimental Methodology}
With our evaluation we try to answer: 

\begin{enumerate}[itemsep=1.5pt]
    \item What overheads are present with remote devices in NBD and NVMe-oF.
    \item How does data granularity affect remote and local options (e.g. \SI{512}{B} and \SI{4}{KB}).
    \item Can SPDK NVMe-oF match up against local drive in big data analytics.
\end{enumerate}

%\note{jk: Make the questions more specific. For example, add a question about
%granularity (e.g. 512b and \SI{4}{KB}). So, think carefully what was the evaluation
%about and write specific questions. Do not care if they are too much, we are
%going to reduce/merge them.}
\vspace{1em}

For the evaluation, we use two servers with Intel Xeon E5-2630 32 Cores @ 2.4
GHz. Each server uses 256GB of DDR4 DRAM divided on two NUMA nodes, each with 16
threads. For storage we use Samsung 970 EVO Plus 2 TB PCIe Gen 3.0 x4 NVMe SSD.
Each server is equipped with Mellanox Technologies ConnectX-3 Network controller
MT27500 with fibre ports compliant with the InfiniBand Architecture
Specification ~\cite{infiniband}. The servers run CentOS version 7.9 with Linux kernel version is
5.4.267. Table \ref{tab:serv_specs} shows the server specifications.
\begin{table}[H]
\resizebox{8.5cm}{1cm}{%
\begin{tabular}{cccccc}
\textbf{ID} & \textbf{CPU}                                                                      & \textbf{DRAM}                                        & \textbf{Device}                                                                       & \textbf{NIC}                                                                       & \textbf{Kernel} \\ \hline
Server 1    & \begin{tabular}[c]{@{}c@{}}Intel Xeon E5-2630\\  32 Cores @ 2.4 GHz.\end{tabular} & \begin{tabular}[c]{@{}c@{}}256GB\\ DDR4\end{tabular} & \begin{tabular}[c]{@{}c@{}}Samsung 970 EVO \\ Plus 2 TB PCIe \\ NVMe SSD\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mellanox Technologies \\ ConnectX-3 MT2750\end{tabular} & 5.4.267         \\ \hline
Server 1    & \begin{tabular}[c]{@{}c@{}}Intel Xeon E5-2630\\  32 Cores @ 2.4 GHz.\end{tabular} & \begin{tabular}[c]{@{}c@{}}256GB\\ DDR4\end{tabular} & \begin{tabular}[c]{@{}c@{}}Samsung 970 EVO \\ Plus 2 TB PCIe \\ NVMe SSD\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mellanox Technologies \\ ConnectX-3 MT2750\end{tabular} & 5.4.267        
\end{tabular}
}
\caption{Server specifications table.}
\label{tab:serv_specs}
\end{table}

\paragraph{Flexible I/O (FIO) Tester.}
FIO ~\cite{fio} is a widely used open-source tool in the Linux ecosystem for benchmarking
and testing various I/O (input/output) workloads on storage devices. FIO
provides detailed output reports, including metrics such as throughput, IOPS
(input/output operations per second), latency, and CPU utilization. We configure FIO to use the
libaio engine, random reads, direct I/O, \SI{512}{B} bytes and \SI{4}{KB} block size, 1 I/O
depth and 1 thread to measure the latency of all the device setups shown in Table
\ref{tab:storage_configurations}. 

\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{3cm}|>{\RaggedRight\arraybackslash}m{5cm}|}
\hline
\textbf{Configuration} & \textbf{Description} \\
\hline
LOC\_n & Local Non-Volatile Memory Express (NVMe) storage drive. \\
\hline
LOC\_r & Storage using a portion of system RAM as a high-speed drive. \\
\hline
NBD\_n & Network Block Device (NBD) configured to use an NVMe drive over the network. \\
\hline
NBD\_r & Network Block Device (NBD) configured to use a RAM-disk over the network. \\
\hline
SPDK\_n & Local NVMe drive managed with Storage Performance Development Kit (SPDK) user-space drivers. \\
\hline
SPDK\_r & Local RAM-disk managed with SPDK user-space drivers. \\
\hline
OF\_n & NVMe over Fabrics (NVMe-oF) using NVMe drives managed with SPDK user-space drivers. \\
\hline
OF\_r & NVMe-oF using RAM-disk managed with SPDK user-space drivers. \\
\hline
\end{tabular}
\caption{Storage device setups.}
\label{tab:storage_configurations}
\end{table}





\paragraph{Netperf.} 
Netperf ~\cite{netperf} is a benchmarking tool used to measure the performance of networking
systems. It's designed to provide a standardized method for measuring networking
performance between two systems. Netperf allows users to test various aspects of
networking performance, such as throughput, latency, and jitter, across
different network protocols like TCP (Transmission Control Protocol) and UDP
(User Datagram Protocol). We will use Netperf to measure end-to-end latency of
TCP (Transmission Control Protocol) To better understand the latency of systems
like Network Block Device (NBD) which uses TCP to export the block device to the
network.

\paragraph{TeraHeap.} TeraHeap ~\cite{teraheap} is a system that eliminates S/D overhead and expensive GC scans for a
large portion of the objects in big data frameworks. TeraHeap enhances the
managed runtime environment, particularly the Java Virtual Machine (JVM). It
introduces a supplementary heap, designed for high-capacity storage, alongside
the primary heap. This secondary heap utilizes fast storage and allows direct
access to objects without the need for serialization or deserialization.
Additionally, TeraHeap minimizes the garbage collection overhead by preventing
the garbage collector from scanning the secondary heap. It takes advantage of
frameworks' capability to designate certain objects for off-heap allocation and
provides them with a hint-based method for relocating these objects to the
secondary heap. We will use TeraHeap as a Real world system. We configure TeraHeap to apply the supplementary heap to local, remote block devices, and remote Ram-disk and compare the performances of each block device system.

\paragraph{ib\_read\_lat.} ib\_read\_lat is a micro-benchmark from the Perftest tool ~\cite{perftest}. specifically designed for measuring InfiniBand (IB) latency. It evaluates the time taken for data to be transferred and received between InfiniBand endpoints. We will use it to measure the latency of transferring data between two Mellanox Technologies ConnectX-3 MT2750 network cards using the IB port.

\paragraph{Workloads and Execution time breakdown.} We employ eight memory-intensive tasks from Spark-Bench ~\cite{spark} and five LDBC Graphalytics suites for Giraph ~\cite{giraph}, generating datasets accordingly. For the remote Ram-disk, we use the five LDBC Graphalytics suites for Giraph and generate a smaller dataset to fit in the limited \SI{100}{G} Ram-disk exported with SPDK NVMe-oF. Each experiment is run five times, and the average end-to-end execution time is recorded. Time breakdown includes 'other' time, S/D plus I/O time, minor GC time, and major GC time. 'Other' time covers mutator threads, potentially including I/O wait. The profiler operates with minimal overhead. We configure TeraHeap to allocate the first heap (H1) on DRAM and the second heap (H2) over a file in Local or Remote block device or Remote Ram-disk via memory-mapped I/O (mmio) we also limmit DRAM and H1 to a fixed size to create pressure and more I/O to H2. Table \ref{tab:workloads} shows the DRAM and H1 size in each workload, in Spark and Giraph, accordingly. 
\begin{table}[H]
\centering
\resizebox{8cm}{3.1cm}{%
\begin{tabular}{|c|l|r|r|}
\hline
\multicolumn{1}{|l|}{\textbf{Frameworks}} & \textbf{Benchmarks} & \multicolumn{1}{c|}{\textbf{TOTAL DRAM (GB)}} & \multicolumn{1}{c|}{\textbf{H1 (GB)}} \\ \hline
\multirow{8}{*}{\rotatebox[origin=c]{90}{\textbf{Spark}}} & Pagerank & 80 & 64 \\ \cline{2-4} 
 & Connected Components & 84 & 68 \\ \cline{2-4} 
 & Linear Regression & 54 & 27 \\ \cline{2-4} 
 & Logistic Regression & 54 & 27 \\ \cline{2-4} 
 & Triangle Counts & 80 & 64 \\ \cline{2-4} 
 & Shortest Path & 58 & 42 \\ \cline{2-4} 
 & SVDPlusPlus & 40 & 24 \\ \cline{2-4} 
 & SVM & 48 & 32 \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Giraph}}} & PageRank & 85 & 50 \\ \cline{2-4} 
 & CDLP & 85 & 60 \\ \cline{2-4} 
 & WCC & 85 & 60 \\ \cline{2-4} 
 & BFS & 65 & 35 \\ \cline{2-4} 
 & SSSP & 90 & 50 \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{\shortstack{\textbf{Giraph}\\\textbf{Ram-disk}}}} & PageRank & 16 & 12 \\ \cline{2-4} 
 & CDLP & 16 & 12 \\ \cline{2-4} 
 & WCC & 16 & 12 \\ \cline{2-4} 
 & BFS & 14 & 10 \\ \cline{2-4} 
 & SSSP & 24 & 20 \\ \hline
\end{tabular}%
}
\caption{TeraHeap workload configuration table.}
\label{tab:workloads}
\end{table}

